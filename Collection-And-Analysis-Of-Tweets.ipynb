{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collecting and Analyzing Tweets\n",
    "\n",
    "To replicate my set up and run this notebook, please visit https://github.com/galletti94/Tweet-Monitor \n",
    "\n",
    "\n",
    "### Let's get some tweets!\n",
    "\n",
    "For this we use the twitter api which requires you to have a twitter account. Twitter gives you access keys which are unique to your account. These keys will go in a file called keys.txt which we make sure to include in the .gitignre file so that these are not made publically available. Let's get started!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "from pymongo import MongoClient\n",
    "from __future__ import print_function\n",
    "\n",
    "MONGO_HOST = 'mongodb://localhost/twitterdb'\n",
    "\n",
    "WORDS = ['#deeplearning', '#computervision', '#datascience', '#bigdata']\n",
    "\n",
    "LOCATION = [-127.3,24.1,-65.9,51.8]\n",
    "\n",
    "# get credentials from the keys.txt file\n",
    "keys_file = open(\"keys.txt\")\n",
    "lines = keys_file.readlines()\n",
    "consumer_key = lines[0].rstrip()\n",
    "consumer_secret = lines[1].rstrip()\n",
    "access_token = lines[2].rstrip()\n",
    "access_token_secret = lines[3].rstrip()\n",
    "keys_file.close()\n",
    "\n",
    "class StreamListener(tweepy.StreamListener):\n",
    "    # This is a class provided by tweepy to access the Twitter Streaming API.\n",
    "\n",
    "    def on_connect(self):\n",
    "        # Called initially to connect to the Streaming API\n",
    "        print(\"You are now connected to the streaming API.\")\n",
    "\n",
    "    def on_error(self, status_code):\n",
    "        print('An Error has occured: ' + repr(status_code))\n",
    "        return False\n",
    "\n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            client = MongoClient(MONGO_HOST)\n",
    "            db = client.twitterdb\n",
    "            \n",
    "            datajson = json.loads(data) # Decode the JSON from Twitter\n",
    "\n",
    "            # grab the 'created_at' data from the Tweet to use for display\n",
    "            created_at = datajson['created_at']\n",
    "            \n",
    "            # only get tweets that have geo location enabled\n",
    "            if datajson['coordinates']:\n",
    "                # print out a message to the screen that we have collected a tweet\n",
    "                print(\"Tweet collected at \" + str(created_at))\n",
    "                db.twitter_search.insert(datajson) #insert into db\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "listener = StreamListener(api=tweepy.API(wait_on_rate_limit=True))\n",
    "streamer = tweepy.Stream(auth=auth, listener=listener)\n",
    "\n",
    "# uncomment the next two lines and comment out the last two if you want to only store United States tweets\n",
    "# print(\"Tracking: \" + 'United States')\n",
    "# streamer.filter(locations=LOCATION)\n",
    "\n",
    "#Tracking tweets that include the word in WORDS\n",
    "print(\"Tracking: \" + str(WORDS))\n",
    "streamer.filter(track=WORDS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome - now we can let this run and collect some tweets!\n",
    "\n",
    "After waiting a few minutes for your database to fill up, you can hit CTRL + C to stop the collection and stop the program.\n",
    "\n",
    "### What do we have here?\n",
    "\n",
    "Let's take a look at the tweets we collected!\n",
    "\n",
    "#### Where are they from?\n",
    "\n",
    "We can use folium to map the coordinates of the tweets on a map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "import folium\n",
    "\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['twitterdb']\n",
    "collection = db['twitter_search']\n",
    "tweets_iterator = collection.find()\n",
    "\n",
    "mymap = folium.Map(location=[45.372, -121.6972], zoom_start=4)\n",
    "\n",
    "for tweet in tweets_iterator:\n",
    "    if tweet['coordinates']:\n",
    "        folium.CircleMarker(location=list(reversed(tweet['coordinates']['coordinates']))).add_to(mymap)\n",
    "    \n",
    "mymap.save('map.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice anything? It seems like very few people that tweet about deeplearning, machine learning etc. have their location enabled! Actually I collected over 10000 tweets without checking for whether location was enabled and it turned out the map produced by folium displayed no point! - none of them had their location enabled\n",
    "\n",
    "Let's compare to tweets in the US without keywords:\n",
    "\n",
    "![Image](./tweetsUSA.jpg)\n",
    "\n",
    "Nice. Now let's look at what kinds of emojis people use!\n",
    "\n",
    "#### Emojis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('‚ù§', 164), ('üî•', 160), ('üéÑ', 121), ('üòç', 118), ('üèæ', 112), ('üèº', 112), ('üòÇ', 104), ('üèª', 90), ('üèΩ', 81), ('‚ú®', 79), ('üôå', 68), ('üí™', 67), ('üíØ', 62), ('üôè', 60), ('‚ùÑ', 60)]\n"
     ]
    }
   ],
   "source": [
    "from emoji import UNICODE_EMOJI\n",
    "\n",
    "client = MongoClient('localhost', 27017)\n",
    "db = client['usa_db']\n",
    "collection = db['usa_tweets_collection']\n",
    "tweets_iterator = collection.find()\n",
    "\n",
    "d = dict()\n",
    "i = 0\n",
    "for tweet in tweets_iterator:\n",
    "  for ch in list(tweet['text']):  #remember emojis are characters not words\n",
    "    if ch in UNICODE_EMOJI:\n",
    "      try:\n",
    "        d[ch] += 1\n",
    "      except KeyError:\n",
    "        d[ch] = 1\n",
    "\n",
    "d = sorted(d.items(), key=lambda x: -x[1])\n",
    "print(d[:15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When this was written it was close to christmas so I guess the second and third emojis make sense.\n",
    "\n",
    "\n",
    "#### Sentiment Analysis\n",
    "\n",
    "\n",
    "We can analyze the words of the tweets to get a sense of whether the tweet has positive sentiment, negative sentiment or neutral sentiment. We can aggregate these and get a sense of the general mood of the US at this time of year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "\n",
    "tweetCnt = 0\n",
    "locEnabled = 0\n",
    "for tweet in tweets_iterator:\n",
    "  if 'data' in tweet['text'].lower():\n",
    "    tweetCnt += 1\n",
    "    if tweet['user']['location']:\n",
    "      locEnabled += 1\n",
    "\n",
    "    blob = TextBlob(tweet['text'], analyzer=NaiveBayesAnalyzer())\n",
    "    if blob.sentiment.classification == 'pos':\n",
    "      print('positive sentiment for the tweet: ', tweet['text'])\n",
    "    if blob.sentiment.classification == 'neg':\n",
    "      print('negative sentiment for the tweet: ', tweet['text'])\n",
    "    if blob.sentiment.classification == 'neu':\n",
    "      print('neutral sentiment for the tweet: ', tweet['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stay tuned for more!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
